{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Projet de fin d'année : Intelligence Artificielle** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sujet : (TP DEEP)**\n",
    "- choisir un autre problème de classification et entraîner un réseau de neurones simple,\n",
    "- reprendre celui sur les chiffres et étudier l'impact d'un jeu de données « faussé »   \n",
    "  en apprentissage (suivant le taux d'inversion entre le chiffre 1 et 7 par exemple).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ce que l'on à choisi de faire :**\n",
    "\n",
    "Nous avons choisi de nous baser sur un dataset déjà présent dans keras, le dataset IMDB.\n",
    "\n",
    "Celui ci contient beaucoup de types de données différentes, mais celles qui vont nous intéresser vont être :  \n",
    "- Le contenu des commentaires d'un film\n",
    "- L'aspect positif ou negatif du commentaire\n",
    "\n",
    "Le modèle Kéras que nous allons créer vas avoir pour tâche de récupérer le contenu d'un commentaire sous un film,   \n",
    "établire les occurences de chaque mots dans le commentaire, et en fonction du nombre d'occurences, déterminer  \n",
    "si le commentaire est plutôt positif ou négatif.  \n",
    "\n",
    "Les \"commentaires\" en eux même sont des array d'entiers, qui correspondent en fait aux positions de mots dans le dictionnaire  \n",
    "IMDB qui fait correspondre à un index un mot. Donc un commentaire du type \"Film génial\" pourrait etre représenté sous la forme list([4, 8])  \n",
    "(c'est un exemple, les indices ne correspondent pas).  \n",
    "\n",
    "Le jeu d'entrainement est donc constitué de commentaires et de leur caractère (positif / negatif soit 0 ou 1), et le jeu de   \n",
    "test est constitué de commentaires également, le modèle ayant pour charge de déterminer leur caractère.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction à LSTM et imports :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Nous avons choisi d'utiliser des couches LSTM, proposées par Keras. \n",
    "Voici un extrait de la documentation associée pour précision :\n",
    "\n",
    "Long Short-Term Memory layer - Hochreiter 1997.\n",
    "\n",
    "See the Keras RNN API guide for details about the usage of RNN API.\n",
    "\n",
    "Based on available runtime hardware and constraints, this \n",
    "layer will choose different implementations (cuDNN-based or pure-TensorFlow) \n",
    "to maximize the performance. If a GPU is available and all the arguments to the \n",
    "layer meet the requirement of the cuDNN kernel (see below for details), the layer \n",
    "will use a fast cuDNN implementation.\n",
    "\n",
    "From : https://keras.io/api/layers/recurrent_layers/lstm/\n",
    "\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Variables globales du projet :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre de commentaires à utiliser pour l'apprentissage\n",
    "max_features = 20000\n",
    "# nombre de mots à utiliser pour l'apprentissage (coupé après 150)\n",
    "maxlen = 250 \n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chargement des données à utiliser dans le modèle :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- START LOADING DATA -----\n",
      "LENGTH OF TRAINING DATA:  25000\n",
      "LENGTH OF TEST DATA:  25000\n",
      "----- END LOADING DATA -----\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('----- START LOADING DATA -----')\n",
    "# recuperation des données\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# verifier la taille des données\n",
    "print(\"LENGTH OF TRAINING DATA: \", len(x_train))\n",
    "print(\"LENGTH OF TEST DATA: \", len(x_test))\n",
    "\n",
    "\n",
    "print('----- END LOADING DATA -----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imdb.load_data permet de charger les tableaux contenant les indices des mots des commentaires.   \n",
    "La fonction retourne donc deux tuples, correspondant aux jeux de données d'entrainement et de test.  \n",
    "\n",
    "Le paramètre num_words est en fait le nombre de commentaires que l'on vas récupérer, soit la taille du dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **On prépare les données pour le traitement :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- START PADDING DATA -----\n",
      "----- END PADDING DATA -----\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "What is a pad sequence?\n",
    "\n",
    "This function transforms a list (of length num_samples) \n",
    "of sequences (lists of integers) into a 2D Numpy array of \n",
    "shape (num_samples, num_timesteps). num_timesteps is either \n",
    "the maxlen argument if provided, or the length of the longest \n",
    "sequence in the list.\n",
    "\"\"\"\n",
    "print('----- START PADDING DATA -----')\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print('----- END PADDING DATA -----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on coupe en effet la longueur de tous les commentaires, pour accélerer la vitesse de traitement.\n",
    "\n",
    "En effet, nous avons remarqué au cours de nos essais que garder 500 mots du commentaire avec notre modèle pouvait  \n",
    "entrainer des durées d'apprentissage allant au dela de 6min30 par epochs, ce que nous trouvions trop long pour un  \n",
    "petit projet. C'est pourquoi on coupe les commentaires à 250 mots de longs, n'impactant que très peu la précision   \n",
    "du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Construction du modèle :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- START BUILDING MODEL -----\n",
      "----- END BUILDING MODEL -----\n"
     ]
    }
   ],
   "source": [
    "print('----- START BUILDING MODEL -----')\n",
    "# Construction du modele\n",
    "model = Sequential()\n",
    "# on utilise une couche d'embedding pour convertir les mots en vecteurs\n",
    "model.add(Embedding(max_features, 12))\n",
    "# on utilise une couche LSTM pour la representation (cf supra pour les explications)\n",
    "\"\"\"\n",
    "Extrait de documentation Keras : \n",
    "Dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. Default: 0. \n",
    "Reccurent Dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. Default: 0. \n",
    "\"\"\"\n",
    "model.add(LSTM(12, dropout=0.2, recurrent_dropout=0.2))\n",
    "# on utilise une couche de sortie pour la classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# on compile le modele en utilisant l'optimiseur 'rmsprop' et la fonction de perte 'binary_crossentropy'\n",
    "# la metrics est la fonction de perte qui est utilisée pour calculer l'erreur\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
    "\n",
    "print('----- END BUILDING MODEL -----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Apprentissage et tests du modèle :**\n",
    "\n",
    "Après avoir fait tourner ce modèle, notre meilleur score était approximativement de 87% de reconnaissance sur un jeu de données de test  \n",
    "inconnues du réseau de neuronnes, ce que nous avons trouvé très satisfaisant au vu du caractère assez subjectif d'un commentaire et d'une émotion humaine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- START FITTING MODEL -----\n",
      "Epoch 1/2\n",
      "782/782 [==============================] - 109s 136ms/step - loss: 0.4427 - binary_accuracy: 0.7971 - val_loss: 0.3408 - val_binary_accuracy: 0.8628\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 107s 137ms/step - loss: 0.2466 - binary_accuracy: 0.9074 - val_loss: 0.3212 - val_binary_accuracy: 0.8739\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.3212 - binary_accuracy: 0.8739\n",
      "----- END FITTING MODEL -----\n",
      "Test score: 0.3211544454097748\n",
      "Test accuracy: 0.8739200234413147\n"
     ]
    }
   ],
   "source": [
    "print('----- START FITTING MODEL -----')\n",
    "# on entraine le modele sur les données d'apprentissage\n",
    "# au vu de la taille des données, on utilise un batch_size de 32\n",
    "# et on fait une epoque de 2 epochs\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=2, validation_data=(x_test, y_test))\n",
    "# on détermine le score du modele sur les données de test\n",
    "score, acc = model.evaluate(x_test, y_test,batch_size=batch_size)\n",
    "print('----- END FITTING MODEL -----')\n",
    "\n",
    "# on affiche le score du modele\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons décidé donc de n'utiliser que 2 époque pour faire tourner le modèle, une époque prenant déjà 1min30,   \n",
    "et le modèle convergeant déjà très vite, faire plus d'époques n'améliore pas vraiment l'accurary du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tests sur un couche Dense :**\n",
    "\n",
    "Nous avons également essayé de faire tourner ce modèle avec des couches Dense, mais la qualité de la reconnaissance était  \n",
    "bien moindre, d'ou l'utilisation des couches LSTM, plus faciles également à faire tourner sur n'importe quelle machine étant  \n",
    "donné qu'elles sont basées sur la puissance de calcul de la machine locale. (voir notebook testingDense.ipynb)\n",
    "\n",
    "En effet, un modèle Dense converge très rapidement, mais sur une reconnaissance à 50% environ, soit une chance sur deux, donc  \n",
    "aléatoire étant donné qu'il n'y à que deux états de sortie possible. En plus de ça, la loss était très élevée."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc975ae9a1f6d54eca13f8b8f99fd620f3e15c2f59ead94411c90f930644d6bd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
